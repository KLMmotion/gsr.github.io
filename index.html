<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Your project description here">
  <meta name="keywords" content="your, keywords, here">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GSR: Learning Structured Reasoning for Embodied Manipulation</title>

  <!-- Add own analytics if needed -->
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;600;700;900&family=Lexend:wght@300;400;600;700;800&display=swap" rel="stylesheet" />

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/your-favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://github.com/KLMmotion">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/KLMmotion">
              A
            </a>
            <a class="navbar-item" href="https://github.com/KLMmotion">
              B
            </a>
            <a class="navbar-item" href="https://github.com/KLMmotion">
              C
            </a>
            <a class="navbar-item" href="https://github.com/KLMmotion">
              D
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="./static/images/logo.jpg" alt="GSR logo" width="180" height="180" />
            <h1 class="title is-1 publication-title">GSR: Learning Structured Reasoning for Embodied Manipulation</h1>

            <!-- <h1 class="title is-1 publication-title">
              <img src="./static/images/_20260129144959_85.jpg" alt="GSR logo" width="36" height="36" style="vertical-align:middle; margin-right:0.5rem;">
              Learning Structured Reasoning for Embodied Manipulation
            </h1> -->
            
            <!-- <h1 class="title is-1 publication-title">
              <img src="./static/images/title_2.png" alt="GSR logo" width="1500" height="1500" style="vertical-align:middle; margin-right:0.5rem;">
            </h1> -->

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://github.com/KLMmotion">Kewei Hu</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/KLMmotion">Michael Zhang</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/KLMmotion">Wei Ying</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/KLMmotion">Tianhao Liu</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/KLMmotion">Guoqiang Hao</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/KLMmotion">Zimeng Li</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/KLMmotion">Wanchan Yu</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/KLMmotion">Jiajian Jin</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/KLMmotion">Fangwen Chen</a><sup></sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/KLMmotion">Hanwen Kang</a><sup></sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup></sup>Kernal Mind</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://github.com/KLMmotion" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/KLMmotion" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                  <a href="https://github.com/KLMmotion" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/KLMmotion" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a class="external-link button is-normal is-rounded is-dark" disabled>
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video id="teaser" controls autoplay muted loop playsinline height="100%">
          <source src="./static/videos/pp0128-480p.mp4" type="video/mp4">
        </video>
        <!-- <h2 class="subtitle has-text-centered">
          <span class="dnerf">GSR</span> blah blah blah
        </h2> -->
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="rows">
        <div class="rows is-centered">
          <div class="column is-centered has-text-centered">
            <h2 class="title is-3">
              Abstract
            </h2>
            <div class="content has-text-justified">

              <p>
                Despite rapid progress, embodied agents still struggle with long-horizon manipulation that requires
                maintaining spatial consistency, causal dependencies, and goal constraints. A key limitation of existing
                approaches is that task reasoning is implicitly embedded in high-dimensional latent representations,
                making it challenging to separate task structure from perceptual variability.
                We introduce Grounded Scene-graph Reasoning (GSR), a structured reasoning paradigm that explicitly
                models world-state evolution as transitions over semantically grounded scene graphs. By reasoning
                step-wise over object states and spatial relations, rather than directly mapping perception to actions,
                GSR enables explicit reasoning about action preconditions, consequences, and goal satisfaction in a
                physically grounded space.
                To support learning such reasoning, we construct Manip-Cognition-1.6M, a large-scale dataset that
                jointly supervises scene grounding, causal action reasoning, and goal-conditioned planning. Extensive
                evaluations across RLBench, LIBERO, GSR-benchmark, and real-world robotic tasks show that GSR
                significantly improves zero-shot generalization and long-horizon task completion over prompting-based
                baselines.
                These results highlight explicit world-state representations as a key inductive bias for scalable
                embodied reasoning.
              </p>
            </div>

            <br>

            <h2 class="title is-3">
              Overview
            </h2>
            <div class="content has-text-justified">
              <p>
                <img src="./static/images/introduction-Overall of dataset and framework.png" width="100%"
                  alt="GSR Framework">
                <b>Overview of GSR Framework.</b>
                To this end, we introduce <strong>Grounded Scene-graph Reasoning (GSR)</strong>,
                an embodied reasoning framework based on the principle that agents should plan over abstract world
                representations rather than raw visual information.
                GSR leverages semantically grounded scene graphs to extract stable causal structures from observations
                and explicitly separates high-level conceptual reasoning from low-level action execution.
                This design enables persistent and task-transferable capabilities, allowing flexible composition of
                sequenced action and robust adaptation across tasks.
                To train GSR, we construct the <strong>Manip-Cognition-1.6M</strong>, which provides joint supervision
                over world understanding, intention interpretation, and action planning across a diverse set of
                manipulation tasks.
              </p>
            </div>

            <br>

            <h2 class="title is-3">
              Methodology
            </h2>
            <div class="content has-text-justified">
              <img src="./static/images/Method-Manip-Cognition dataset examples.png" width="100%" alt="GSR Framework">
              <p class="has-text-justified">
                Given an RGB-D observation \(\mathcal{I}\), we construct a 3D scene graph \(M_{sg} = (O_t, E_t)\) to encode
                the workspace and object states, where \(O_t = \{o_j\}_{j=1,...,J}\) denotes the set of objects and \(E_t =
                \{e_k\}_{k=1,...,K}\) denotes the set of relational edges.
                Figure above illustrates the transformation process from raw visual input to a structured
                representation and the resulting scene graph.
                Each object \(o_j\) is represented as a structured entity composed of functional keypoints and,
                when applicable, articulated child components.
                For example, a <strong>mug</strong> includes a "<em>functional keypoint</em>" corresponding to its
                <strong>handle</strong>,
                while a <strong>cabinet</strong> is modeled as an articulated object with "<em>multiple child elements</em>"
                such as <strong>drawers</strong>.
                Edges \(e_k\) encode spatial relations between object pairs, capturing predicates such as <em>on</em>,
                <em>inside</em>, or <em>adjacent</em> to (e.g., a <strong>mug</strong> <em>on</em> a <strong>table</strong>).
              </p>
              <p class="has-text-justified">
                GSR is a fine-tuned Large Language Model (LLM) designed to perform commonsense reasoning over
                scene-graph representations.
                To apply GSR in a physical embodiment, we integrate it with a perception front-end and a action expert
                back-end.
                The physical system consists of two components: a perception-reasoning module for decision making, and
                an action expert that executes low-level control.
                The perception-reasoning module constructs scene graphs from raw observations and enable GSR reasons
                over these information to generate sequences of actions.
                To construct scene graphs, an <strong>Vision Foundation Model (VFM)</strong> is applied.
                The action expert leverages a meta-skill library, with further details described in the paper.
              </p>
            </div>
          </div>
        </div>

        <br>

        <div class="column is-centered">
          <h2 class="title is-3 has-text-centered">
            Experiments
          </h2>
          
          <br>
          
          <h2 class="title is-4">
            Libero Benchmarks
          </h2>

          <p class="has-text-justified">
            This experiment evaluates the model's <strong>general reasoning capability</strong> in <strong>manipulation tasks without task-specific training</strong>.
            The model must generate the <strong>command</strong> conditioned on given <strong>initial state</strong>, infer the underlying <strong>goal</strong>, and predicts the correct <strong>actions</strong>. 
            We select five LLMs as baselines, including <code>GPT-5</code>, <code>Gemini-2.5-Pro</code>, <code>DeepSeek-V3</code>, <code>Qwen3-8B</code>, and <code>Claude-Sonnet-4.5</code>.
            To deploy LLMs in manipulation tasks, we integrate a scene-graphâ€“based perception module with an LLM control interface, which maps language commands to corresponding low-level actions.
            We evaluate GSR in a zero-shot setting on RLBench and LIBERO without any fine-tuning. For the other models, we design task-specific prompts for each LLM to ensure their performance is well optimized during evaluation. 
            On <strong>RLBench</strong>, we evaluate 100 tasks across five representative categories: <strong>Kitchen Operations (KO)</strong>, <strong>Pick and Place (PP)</strong>, <strong>Switch Operations for Containers (SC)</strong>, <strong>Controller Operations (CO)</strong>, and <strong>Stacking and Assembly (SA)</strong>.
            On <strong>LIBERO</strong>, we evaluate 100 tasks across four subsets: <strong>LIBERO-Spatial</strong>, <strong>LIBERO-Object</strong>, <strong>LIBERO-Goal</strong>, and <strong>LIBERO-Long</strong>.
            We run ten trials per task and report the <strong>success rate</strong>. 
            Detailed results are reported in the accompanying tables and figures in the paper.
          </p>

          <br><br>

          <div class="columns is-centered">

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 1</h4> -->
                <p class="mb-3">
                  Pick up the book and place it in the back compartment of the caddy
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source
                    src="./static/videos/libero_10/1pick_up_the_book_and_place_it_in_the_back_compartment_of_the_caddy.mp4"
                    type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 2</h4> -->
                <p class="mb-3">
                  Put both the cream cheese box and the butter in the basket
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source
                    src="./static/videos/libero_10/1put_both_the_cream_cheese_box_and_the_butter_in_the_basket.mp4"
                    type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 3</h4> -->
                <p class="mb-3">
                  Put the black bowl in the bottom drawer of the cabinet and close it
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source
                    src="./static/videos/libero_10/1put_the_black_bowl_in_the_bottom_drawer_of_the_cabinet_and_close_it.mp4"
                    type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

          </div>

          <br>

          <div class="columns is-centered">

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 4</h4> -->
                <p class="mb-3">
                  Put the white mug on the plate and put the chocolate pudding to the right of the plate
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source
                    src="./static/videos/libero_10/1put_the_white_mug_on_the_plate_and_put_the_chocolate_pudding_to_the_right_of_the_plate.mp4"
                    type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 5</h4> -->
                <p class="mb-3">
                  Put the yellow and white mug in the microwave and close it
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source
                    src="./static/videos/libero_10/1put_the_yellow_and_white_mug_in_the_microwave_and_close_it.mp4"
                    type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 6</h4> -->
                <p class="mb-3">
                  Turn on the stove and put the moka pot on it
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/libero_10/1turn_on_the_stove_and_put_the_moka_pot_on_it.mp4"
                    type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

          </div>

        </div>

        <br><br>

        <div class="column is-centered">
          <h2 class="title is-4">
            GSR Benchmarks
          </h2>

          <p class="has-text-justified">
            <strong>GSR-Bench</strong> evaluates long-horizon task reasoning under both spatial and semantic constraints. 
            It consists of 180 tasks, with an average planning horizon exceeding 10 steps. 
            Our evaluation focuses on three aspects: 
            <strong>(1) Semantic Object Disambiguation (SOD)</strong>, which assesses reasoning under varying object semantics; 
            <strong>(2) Spatial-Aware Sequencing (SAS)</strong>, which evaluates reasoning over physical causality and spatial constraints; 
            and <strong>(3) Goal-conditioned Generalization (GCG)</strong>, which measures reasoning across diverse abstract goals. 
            For each aspect, we define three difficulty levels: <em>easy</em>, <em>medium</em>, and <em>difficult</em>
          </p>
          
          <br><br>

          <div class="columns is-centered">

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 1</h4> -->
                <p class="mb-3">
                  GSR Object Simple
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/GSR/obj simple.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 2</h4> -->
                <p class="mb-3">
                  GSR Object General
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/GSR/obj general.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 3</h4> -->
                <p class="mb-3">
                  GSR Object Difficult
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/GSR/obj-difficult.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

          </div>

          <br>

          <div class="columns is-centered">

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 4</h4> -->
                <p class="mb-3">
                  GSR Spatial Simple
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/GSR/spatial simple.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 5</h4> -->
                <p class="mb-3">
                  GSR Spatial General
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/GSR/spatial-general.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 6</h4> -->
                <p class="mb-3">
                  GSR Spatial Difficult
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/GSR/spatial-difficult.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

          </div>

          <br>

          <div class="columns is-centered">

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 7</h4> -->
                <p class="mb-3">
                  GSR Goal Simple
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/GSR/goal-simple.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 8</h4> -->
                <p class="mb-3">
                  GSR Goal General
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/GSR/goal general.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 9</h4> -->
                <p class="mb-3">
                  GSR Goal Difficult
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/GSR/goal-difficult.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

          </div>

        </div>

        <br><br>

        <div class="column is-centered">
          <h2 class="title is-4">
            Real World Results
          </h2>
          <p class="has-text-justified">
            This experiment evaluates GSR in three real-world settings, each targeting a distinct aspect of its reasoning and generalization capabilities. 
            First, we assess GSR on general pick-and-place tasks involving common objects, evaluating its reasoning ability to generate appropriate behaviors in response to natural language commands. 
            Second, we evaluate GSR on long-horizon sorting tasks as in GSR-Bench, examining its performance across diverse spatial configurations and goal conditions that require extended planning. 
            Third, we demonstrate GSR on four daily-life tasks, detailed results are shown in the supplementary videos.
          </p>
          <br><br>
          
          <div class="columns is-centered">
            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 1</h4> -->
                <p class="mb-3">
                  Packaging a cardboard box
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/real/cardboard_box.mp4?v=2" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 2</h4> -->
                <p class="mb-3">
                  Placing a cup under the coffee machine
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/real/coffee_machine.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 3</h4> -->
                <p class="mb-3">
                  Unzipping a backpack
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/real/fe94bf1a5c3bafaa3d01f0661eb8ffc2.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

          </div>

          <br>

          <div class="columns is-centered">
            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 4</h4> -->
                <p class="mb-3">
                  Sorting colored cubes
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/real/colored_cubes.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 5</h4> -->
                <p class="mb-3">
                  Tucking pens into a pencil pouch
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/real/pencil_case.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 6</h4> -->
                <p class="mb-3">
                  Pouring water
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/real/ded80fc322f0736cd7b213921ba2038d.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

          </div>

          <br>

          <div class="columns is-centered">
            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 7</h4> -->
                <p class="mb-3">
                  Following human instructions
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/real/Follow human instructions.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 8</h4> -->
                <p class="mb-3">
                  Picking and placing items into a drawer
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/real/real task drawer.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

            <!-- video -->
            <div class="column">
              <div class="content">
                <!-- <h4 class="title is-5">Task 9</h4> -->
                <p class="mb-3">
                  Heating bread in a microwave
                </p>
                <video id="id" autoplay controls muted loop playsinline height="100%">
                  <source src="./static/videos/real/real task mic.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!-- /video -->

          </div>

        </div>
      </div>
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">static/videos/libero_10/1put_both_the_cream_cheese_box_an
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>